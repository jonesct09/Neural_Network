# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dcJdK_eNWFl6nOxCCARHEddNnnVAQz0A
"""

### the base code (no tests)# import math
# from matplotlib import pyplot as plt
# import numpy as np
# import random
# import copy

# layers_neurons = [2, 3, 1]
# lln = len(layers_neurons) - 1

# weights = {}
# layer_outs = {}
# layer_outs["Y_T"] = Y
# layer_outs["a0"] = X

# def leaky_relu(ins):
#   for inn in ins:
#     for innn in inn:
#       if innn >= 0:
#         pass
#       else:
#         innn = innn/2
#   return(ins)

# def derivative_lr(ins):
#   for inn in ins:
#     for innn in inn:
#       if innn >= 0:
#         innn = 1
#       else:
#         innn = 1/2
#   return(ins)

# def initialize_network():
#   for layer in range(len(layers_neurons)):
#     if layer != 0:
#       neurons = layers_neurons[layer]
#       weights['b' + str(layer)] = np.zeros((neurons))
#       weights['w' + str(layer)] = np.random.randn(neurons, layers_neurons[layer-1])

# def forward(weights):
#   for layer in range(len(layers_neurons)):
#     pass



# # initialize_network()
# # Y_initial = forward(weights)

# def backprop():
#   forward(weights)
#   gradients = {}
#   loss = np.square(layer_outs['Y_T'] - layer_outs['Y'])
#   dldy = layer_outs['Y'] - layer_outs['Y_T']
#   cost = 0
#   dldzlln = dldy * derivative_lr(layer_outs['z' + str(lln)])

#   for weight_name in reversed(weights):

#     weight_num = int(weight_name.strip('bw'))

#     pa = layer_outs['a' + str(int(weight_num) - 1)]

#     if weight_num == 1:
#       if lln == 1:
#         dldz = dldzlln
#       else:
#         dlda = np.dot(gradients['z' + str(weight_num + 1)], weights['w' + str(weight_num + 1)])
#         dldz = dlda * derivative_lr(layer_outs['z' + str(weight_num)])

#     elif weight_num == lln and lln != 1:
#       dldz = dldzlln

#     else:
#       dlda = np.dot(gradients['z' + str(weight_num + 1)], weights['w' + str(weight_num + 1)])
#       dldz = dlda * derivative_lr(layer_outs['z' + str(weight_num)])

#     if 'w' in weight_name and weight_num != 1:
#       gradients['z' + str(weight_num)] = dldz
#       gradients[weight_name] = np.dot(pa.T, dldz)

#     elif 'w' in weight_name and weight_num == 1:
#       gradients['z' + str(weight_num)] = dldz
#       gradients[weight_name] = np.dot(dldz.T, X)

#     if 'b' in weight_name:
#       gradients[weight_name] = np.dot(dldz.T, np.ones((N_points)))

#   return gradients, cost










### this is what I came up with:
#   I changed some variable names
#   ran a simple xor gate test to see if it works
#   found out what was happening with the 0.5's output
#   NEW PROBLEM: Be careful of exploding gradients
#   I couldn't get the original test to work
#   look at my code at the file I shared called mnistTest, I commented on the backprop

import numpy as np
import math
# import random
from matplotlib import pyplot as plt
# import copy

# structure and network hyperparameters
# note: I found that more layers helps stop it from returning all 0.5's
#       The 0.5's are because some of the outputs of the neurons are negative, and the leaky relu does really bad with negative numbers,
#       so more neurons makes it possible to mitigate the others that only return negative numbers
layers_neurons = [2, 12, 1]
num_layers = len(layers_neurons) - 1

weights = {}
layer_outputs = {}

# XOR inputs and outputs
X = [
    [0, 0],
    [1, 0],
    [0, 1],
    [1, 1],
]
Y = [
  [0],
  [1],
  [1],
  [0],
]

# def generate_dataset(N_points):
#     # 1 class
#     radiuses = np.random.uniform(0, 0.5, size=N_points//2)
#     angles = np.random.uniform(0, 2*math.pi, size=N_points//2)

#     x_1 = np.multiply(radiuses, np.cos(angles)).reshape(N_points//2, 1)
#     x_2 = np.multiply(radiuses, np.sin(angles)).reshape(N_points//2, 1)
#     X_class_1 = np.concatenate((x_1, x_2), axis=1)
#     Y_class_1 = np.full((N_points//2,), 1)

#     # 0 class
#     radiuses = np.random.uniform(0.6, 1, size=N_points//2)
#     angles = np.random.uniform(0, 2*math.pi, size=N_points//2)

#     x_1 = np.multiply(radiuses, np.cos(angles)).reshape(N_points//2, 1)
#     x_2 = np.multiply(radiuses, np.sin(angles)).reshape(N_points//2, 1)
#     X_class_0 = np.concatenate((x_1, x_2), axis=1)
#     Y_class_0 = np.full((N_points//2,), 0)

#     X = np.concatenate((X_class_1, X_class_0), axis=0)
#     Y = np.concatenate((Y_class_1, Y_class_0), axis=0)
#     Y = Y.reshape(N_points, 1)
#     return X, Y


# N_points = 200
# X, Y = generate_dataset(N_points)

def leaky_relu(inputs):
    return np.where(inputs >= 0, inputs, inputs / 2) # note: 0.5 is very high for alpha, but with a simple xor test, it won't do anything bad
    # the usual alpha value is in the range [0.1, 0.01]
    # return np.where(inputs >= 0, inputs, inputs * 0.1)

def derivative_leaky_relu(inputs):
    return np.where(inputs >= 0, 1, 0.5)
    # return np.where(inputs >= 0, 1, 0.1)

def initialize_network():
    for layer in range(1, len(layers_neurons)):
        neurons_in_previous_layer = layers_neurons[layer - 1]
        neurons_in_current_layer = layers_neurons[layer]

        weights[f'b{layer}'] = np.zeros(neurons_in_current_layer)
        # weights[f'w{layer}'] = np.random.randn(neurons_in_current_layer, neurons_in_previous_layer)
        # this is He initialation, It is very alpha sigma
        #   it works very well with relu
        weights[f'w{layer}'] = np.random.randn(neurons_in_current_layer, neurons_in_previous_layer) * np.sqrt(2.0/neurons_in_previous_layer)

def forward():
    # numpy-ify forward
    layer_outputs[f'a{0}'] = X # make X the first layer output

    for layer in range(1, len(layers_neurons)):
        prev_output = layer_outputs[f'a{layer - 1}']
        weights_layer = weights[f'w{layer}']
        bias_layer = weights[f'b{layer}']

        z = np.dot(prev_output, weights_layer.T) + bias_layer
        a = leaky_relu(z)

        layer_outputs[f'z{layer}'] = z
        layer_outputs[f'a{layer}'] = a

def backpropagation():
    gradients = {}

    layer_outputs[f'a{0}'] = X # make X the first layer output

    # Mean squared error
    loss = np.square(Y - layer_outputs['a2']) * 2
    cost = np.sum(loss)

    # partial derivative in respect the the output
    dL_dy = layer_outputs['a2'] - Y

    # I took the last layer derivative out of the loop so I don't have to use an if else check
    # last layer error
    # chains the partial derivative with respect to the output activations
    dL_dz = dL_dy * derivative_leaky_relu(layer_outputs['z2'])

    # the actual backward pass
    for layer_num in reversed(range(1, num_layers + 1)):
        prev_activation = layer_outputs[f'a{layer_num - 1}']

        gradients[f'z{layer_num}'] = dL_dz

        gradients[f'w{layer_num}'] = np.dot(dL_dz.T, prev_activation) # this chains the partial derivative in respect to the weights
        gradients[f'b{layer_num}'] = np.sum(dL_dz, axis=0)

        if layer_num > 1:
            # calculates the derivatives in respect to the layer's error, and the derivative in respect to the layer's activations
            dL_dz = np.dot(dL_dz, weights[f'w{layer_num}']) * derivative_leaky_relu(layer_outputs[f'z{layer_num - 1}'])

    return gradients, cost



initialize_network()

# learn params
learning_rate = 0.03
num_epochs = 1000

# mattplotlib stuff
plotData = []
finalCost = 0

for epoch in range(num_epochs):
    forward()
    gradients, cost = backpropagation()

    for layer_num in range(1, num_layers + 1):
        weights[f'w{layer_num}'] -= learning_rate * gradients[f'w{layer_num}']
        weights[f'b{layer_num}'] -= learning_rate * gradients[f'b{layer_num}']
        plotData += [cost]
        finalCost = cost

print("xor test")
print(f'finalCost: {finalCost}')
plt.plot(plotData)