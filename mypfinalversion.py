# -*- coding: utf-8 -*-
"""MYPFinalVersion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/152XMT3iiGXTs6PltWE-CusEnTXTZauym
"""

import tensorflow
from tensorflow.keras.datasets import mnist
import copy

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Print shapes of the dataset
print(f"Training data shape: {x_train.shape}")
print(f"Testing data shape: {x_test.shape}")


import numpy as np
import math
from matplotlib import pyplot as plt

X = []
Y = []


for i in range(1000):
  y = np.zeros(10)
  y[y_train[i]] = 1
  x_train[i] = np.divide(x_train[i], 255)
  X.append(np.reshape([x_train[i]], (784)))

  Y.append(y)


layers_neurons = [784, 12, 10]
num_layers = len(layers_neurons)
lln = len(layers_neurons) - 1

"""def generate_dataset(N_points):
  # 1 class
  radiuses = np.random.uniform(0, 0.5, size=N_points//2)
  angles = np.random.uniform(0, 2*math.pi, size=N_points//2)

  x_1 = np.multiply(radiuses, np.cos(angles)).reshape(N_points//2, 1)
  x_2 = np.multiply(radiuses, np.sin(angles)).reshape(N_points//2, 1)
  X_class_1 = np.concatenate((x_1, x_2), axis=1)
  Y_class_1 = np.full((N_points//2,), 1)

  # 0 class
  radiuses = np.random.uniform(0.6, 1, size=N_points//2)
  angles = np.random.uniform(0, 2*math.pi, size=N_points//2)

  x_1 = np.multiply(radiuses, np.cos(angles)).reshape(N_points//2, 1)
  x_2 = np.multiply(radiuses, np.sin(angles)).reshape(N_points//2, 1)
  X_class_0 = np.concatenate((x_1, x_2), axis=1)
  Y_class_0 = np.full((N_points//2,), 0)

  X = np.concatenate((X_class_1, X_class_0), axis=0)
  Y = np.concatenate((Y_class_1, Y_class_0), axis=0)
  Y = Y.reshape(N_points, 1)
  return X, Y


N_points = 200
X, Y = generate_dataset(N_points)
"""
weights = {}
layer_outs = {}
layer_outs['Y_T'] = Y[0]
layer_outs['a0'] = X[0]

def leaky_relu(inn):
  return np.maximum(inn*0.1, inn)
def lr_derivative(inn):
  return np.where(inn > 0, 1.0, 0.1)

def softmax(ins):
  exp_values = np.exp(ins - np.max(ins))
  return exp_values / np.sum(exp_values)
def derivative_softmax(ins):
  exp_values = np.exp(ins - np.max(ins))
  sum_exp = np.sum(exp_values)
  return (exp_values * (sum_exp - exp_values)) / (sum_exp ** 2)


def initialize_network():
  for layer in range(len(layers_neurons)):
    if layer != 0:
      neurons = layers_neurons[layer]
      weights['b' + str(layer)] = np.zeros((neurons))
      weights['w' + str(layer)] = ((np.random.randn(layers_neurons[layer-1], neurons) / 2.5) -3) * (1 / (layers_neurons[layer-1]))

initialize_network()

def forward(weights):
  for layer in range(1, len(layers_neurons)):
    layer_num = layer
    neurons = layers_neurons[layer]
    layer_ins = layer_outs['a' + str(layer-1)]
    layer_outs['z' + str(layer)] = z = np.dot(layer_ins, weights["w" + str(layer_num)]) + weights["b" + str(layer_num)]
    layer_outs['a' + str(layer)] = a = leaky_relu(z)
    if layer == lln:
      layer_outs['a' + str(layer)] = a = softmax(z)
      layer_outs['Y'] = a
      return a

initialize_network()
forward(weights)

def backprop():
  outs = forward(weights)
  out = np.argmax(outs)
  t_out = np.argmax(layer_outs["Y_T"])
  if out == t_out:
    loss = 0
  else:
    loss = 100
  gradients = {}

  gradients['z' + str(lln)] = dldzlln = 2 * (layer_outs['Y'] - layer_outs['Y_T']) * derivative_softmax(layer_outs['Y'])

  for layer in range(num_layers - 2, 0, -1):
    gradients['z' + str(layer)] = dldz = np.dot(gradients['z' + str(layer + 1)], weights['w' + str(layer + 1)].T) * lr_derivative(layer_outs['z' + str(layer)])
  for layer in range(1, num_layers):
    gradients['w' + str(layer)] = np.outer(layer_outs['a' + str(layer - 1)], gradients['z' + str(layer)])
    gradients['b' + str(layer)] = np.sum(gradients['z' + str(layer)], axis = 0)


  return gradients, loss

epochs = 300
epsilon = 0.002

epoch_losses = []
batch_losses = []
losses = []
gradients1, loss1 = backprop()
batch_gradients = copy.deepcopy(gradients1)
for weight_set in batch_gradients:
  weight_set = np.zeros_like(weight_set)
batch_num = 200

for epoch in range(epochs):
  epsilon *= 0.98
  indicies = np.random.permutation(len(Y))
  for index, i in zip(indicies, range(len(indicies))):
    x = X[index]
    y = Y[index]
    layer_outs['Y_T'] = y
    layer_outs['a0'] = x
    gradients, loss = backprop()
    losses.append(loss)
    for weight_name in weights:
      batch_gradients[weight_name] += gradients[weight_name]
    if i % batch_num == 0:
      for weight_name in weights:
        weights[weight_name] -= batch_gradients[weight_name] * epsilon
      batch_losses.append(np.average(losses))
      losses.clear()
  epoch_losses.append(np.average(batch_losses))
  batch_losses.clear()


accuracy = 0
for i in range(len(Y)):
    x = X[i]
    y = Y[i]
    layer_outs['Y_T'] = y
    layer_outs['a0'] = x
    forward(weights)
    accuracy += np.argmax(layer_outs['a' + str(lln)]) == np.argmax(y)
accuracy /= len(Y)


plt.plot(epoch_losses)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Curve')
plt.show()
print(f'lowest loss was: {epoch_losses[np.argmin(epoch_losses)]}%')
print(f'accuracy was: {accuracy * 100}%')